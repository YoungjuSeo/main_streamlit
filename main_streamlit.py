import streamlit as st
import os
import tempfile
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.callbacks.base import BaseCallbackHandler

# 0. .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# 1. ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¸ë“¤ëŸ¬
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# 2. PDF ë¡œë“œ ë° ë¬¸ì„œ ë¶„í•  í•¨ìˆ˜
def pdf_to_document(uploaded_file):
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_filepath, "wb") as f:
            f.write(uploaded_file.getvalue())
        loader = PyPDFLoader(temp_filepath)
        pages = loader.load_and_split()
    return pages

# --- Streamlit UI ---
st.set_page_config(page_title="ChatPDF ğŸ“®", layout="centered")
st.title("ChatPDF ğŸ“®")
st.write("---")

# íŒŒì¼ ì—…ë¡œë“œ í•„ë“œ
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])

# API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆê³  íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš° ì‹¤í–‰
if uploaded_file:
    if not api_key:
        st.error(".env íŒŒì¼ì— OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        # ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° DB êµ¬ì¶•
        with st.spinner("ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            pages = pdf_to_document(uploaded_file)
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            texts = text_splitter.split_documents(pages)
            
            embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large")
            vector_db = Chroma.from_documents(documents=texts, embedding=embeddings_model)
            retriever = vector_db.as_retriever()

        st.success("ë¶„ì„ ì™„ë£Œ!")
        st.header("PDFì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
        question = st.text_input("ì§ˆë¬¸ ë‚´ìš©")

        if st.button("ì§ˆë¬¸í•˜ê¸°"):
            if question:
                chat_box = st.empty()
                stream_handler = StreamHandler(chat_box)
                
                llm = ChatOpenAI(
                    model="gpt-4o-mini", 
                    temperature=0, 
                    streaming=True, 
                    callbacks=[stream_handler]
                )

                template = """You are an assistant for question-answering tasks. 
                Use the following pieces of retrieved context to answer the question. 
                If you don't know the answer, just say that you don't know. 
                Keep the answer concise.
                
                Question: {question} 
                Context: {context} 
                Answer:"""
                prompt = ChatPromptTemplate.from_template(template)

                def format_docs(docs):
                    return "\n\n".join(doc.page_content for doc in docs)

                rag_chain = (
                    {"context": retriever | format_docs, "question": RunnablePassthrough()}
                    | prompt
                    | llm
                    | StrOutputParser()
                )
                
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    rag_chain.invoke(question)