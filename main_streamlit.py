import streamlit as st
import os
import tempfile
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.callbacks.base import BaseCallbackHandler

# 0. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# 1. ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ ì •ì˜
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# 2. PDF ë¬¸ì„œ ë¡œë“œ ë° ë¶„í•  í•¨ìˆ˜
def pdf_to_document(uploaded_file):
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_filepath, "wb") as f:
            f.write(uploaded_file.getvalue())
        loader = PyPDFLoader(temp_filepath)
        pages = loader.load_and_split()
    return pages

# --- Streamlit UI ì„¤ì • ---
st.set_page_config(page_title="ChatPDF Bot ğŸ¤–", layout="centered")
st.title("ChatPDF Bot ğŸ¤–")
st.write("---")

# 3. ì„¸ì…˜ ìƒíƒœ(ëŒ€í™” ë‚´ì—­) ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# 4. íŒŒì¼ ì—…ë¡œë“œ ë° ë°ì´í„° ì²˜ë¦¬ (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰ë˜ë„ë¡ ì„¸ì…˜ í™œìš©)
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])

if uploaded_file:
    if "retriever" not in st.session_state:
        with st.spinner("ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            pages = pdf_to_document(uploaded_file)
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            texts = text_splitter.split_documents(pages)
            
            embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large")
            vector_db = Chroma.from_documents(documents=texts, embedding=embeddings_model)
            st.session_state.retriever = vector_db.as_retriever()
            st.success("ë¶„ì„ ì™„ë£Œ! ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")

    # 5. ê¸°ì¡´ ëŒ€í™” ë‚´ì—­ ì¶œë ¥
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 6. ì±„íŒ… ì…ë ¥ ë° ë‹µë³€ ìƒì„±
    if prompt := st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # AI ë‹µë³€ ìƒì„± ê³µê°„ í™•ë³´
        with st.chat_message("assistant"):
            chat_box = st.empty()
            stream_handler = StreamHandler(chat_box)
            
            llm = ChatOpenAI(
                model="gpt-4o-mini", 
                temperature=0, 
                streaming=True, 
                callbacks=[stream_handler]
            )

            template = """You are an assistant for question-answering tasks. 
            Use the following pieces of retrieved context to answer the question. 
            If you don't know the answer, just say that you don't know. 
            Keep the answer concise.
            
            Question: {question} 
            Context: {context} 
            Answer:"""
            prompt_template = ChatPromptTemplate.from_template(template)

            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)

            # RAG ì²´ì¸ êµ¬ì„±
            rag_chain = (
                {"context": st.session_state.retriever | format_docs, "question": RunnablePassthrough()}
                | prompt_template
                | llm
                | StrOutputParser()
            )
            
            # ë‹µë³€ ì‹¤í–‰
            full_response = rag_chain.invoke(prompt)
            st.session_state.messages.append({"role": "assistant", "content": full_response})